{
  "job_name" : "union_people_cnt_in_areas",
  "api_endpoints" : [ ],
  "sql" : "\t\n\t(select TUMBLE_END(eventTimestamp, INTERVAL '10' SECOND) AS window_end,\n\t    SUM( cast(peopleCnt as integer) ) AS total_cnt, 37.500726 as pos_lat, 127.0376948 as pos_lon\n\tfrom \n\t  (\n\t    select person_id, latitude, longitude,  AGGPEOPLECOUNT(cast (37.500726 as string), cast (127.0376948 as string), cast (10000 as string), cast(latitude as string), cast (longitude as string)) as peopleCnt, eventTimestamp\n\t    from person_location_tb\n\t  ) pAgg\n\tGROUP BY\n\t    TUMBLE(eventTimestamp, INTERVAL '10' SECOND)\n\tHAVING\n\t    SUM( cast(peopleCnt as integer) ) > 10\n\t)\n\tUNION ALL\n\t(select TUMBLE_END(eventTimestamp, INTERVAL '10' SECOND) AS window_end,\n\t    SUM( cast(peopleCnt as integer) ) AS total_cnt, 37.534967 as pos_lat, 126.993913 as pos_lon\n\tfrom \n\t  (\n\t    select person_id, latitude, longitude,  AGGPEOPLECOUNT(cast (37.534967 as string), cast (126.993913 as string), cast (10000 as string), cast(latitude as string), cast (longitude as string)) as peopleCnt, eventTimestamp\n\t    from person_location_tb\n\t  ) pAgg\n\tGROUP BY\n\t    TUMBLE(eventTimestamp, INTERVAL '10' SECOND)\n\tHAVING\n\t    SUM( cast(peopleCnt as integer) ) > 10\n\t)\n\tUNION\n\t(select TUMBLE_END(eventTimestamp, INTERVAL '10' SECOND) AS window_end,\n\t    SUM( cast(peopleCnt as integer) ) AS total_cnt, 37.555881 as pos_lat, 126.93295 as pos_lon\n\tfrom \n\t  (\n\t    select person_id, latitude, longitude,  AGGPEOPLECOUNT(cast (37.500726 as string), cast (126.93295 as string), cast (10000 as string), cast(latitude as string), cast (longitude as string)) as peopleCnt, eventTimestamp\n\t    from person_location_tb\n\t  ) pAgg\n\tGROUP BY\n\t    TUMBLE(eventTimestamp, INTERVAL '10' SECOND)\n\tHAVING\n\t    SUM( cast(peopleCnt as integer) ) > 10\n\t)\n",
  "mv_config" : {
    "name" : "union_people_cnt_in_areas",
    "retention" : 300,
    "min_row_retention_count" : 0,
    "recreate" : false,
    "key_column_name" : null,
    "column_indices_disabled" : false,
    "indexed_columns" : [ ],
    "not_indexed_columns" : [ ],
    "api_key" : null,
    "ignore_nulls" : false,
    "require_restart" : false,
    "batch_size" : 0,
    "enabled" : false
  },
  "runtime_config" : {
    "execution_mode" : "SESSION",
    "parallelism" : 1,
    "sample_interval" : 1000,
    "sample_count" : 100,
    "window_size" : 100,
    "start_with_savepoint" : false,
    "log_config" : {
      "type" : "LOG4J_PROPERTIES",
      "content" : "\n################################################################################\n#  Licensed to the Apache Software Foundation (ASF) under one\n#  or more contributor license agreements.  See the NOTICE file\n#  distributed with this work for additional information\n#  regarding copyright ownership.  The ASF licenses this file\n#  to you under the Apache License, Version 2.0 (the\n#  \"License\"); you may not use this file except in compliance\n#  with the License.  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n# limitations under the License.\n################################################################################\n\n# This affects logging for both user code and Flink\nrootLogger.level = INFO\nrootLogger.appenderRef.file.ref = MainAppender\n\n# Uncomment this if you want to _only_ change Flink's logging\n#logger.flink.name = org.apache.flink\n#logger.flink.level = INFO\n\n# The following lines keep the log level of common libraries/connectors on\n# log level INFO. The root logger does not override this. You have to manually\n# change the log levels here.\nlogger.akka.name = akka\nlogger.akka.level = INFO\nlogger.kafka.name= org.apache.kafka\nlogger.kafka.level = INFO\nlogger.hadoop.name = org.apache.hadoop\nlogger.hadoop.level = INFO\nlogger.zookeeper.name = org.apache.zookeeper\nlogger.zookeeper.level = INFO\n\n# Log all infos in the given file\nappender.main.name = MainAppender\nappender.main.type = RollingFile\nappender.main.append = false\nappender.main.fileName = ${sys:log.file}\nappender.main.layout.type = PatternLayout\nappender.main.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\nappender.main.filePattern = ${sys:log.file}-archive-%d{yyyyMMdd_HHmmss}.log\nappender.main.policies.type = Policies\nappender.main.policies.size.type = SizeBasedTriggeringPolicy\nappender.main.policies.size.size = 100MB\n\n# Suppress the irrelevant (wrong) warnings from the Netty channel handler\nlogger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline\nlogger.netty.level = OFF\n"
    }
  }
}